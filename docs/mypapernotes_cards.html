
    <head>
        <style>
            table {
                --color: #d0d0f5;
            }

            thead,
            tfoot {
                background: var(--color);
            }

            tbody tr:nth-child(even) {
                background: color-mix(in srgb, var(--color), transparent 60%);
            }
            div, p, td {
                width: 200px;
                word-wrap: break-word;
                white-space: normal;
            }
            .fixwtd {
                width: 200px;
                word-wrap: break-word;
                white-space: normal;
            }
            .indextd {
                width: 10px;
                word-wrap: break-word;
                white-space: normal;
            }
        </style>
    </head>
    <body>
        <table>
          <tr><td class="indextd">0</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.get_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="î§‹" href="torch.nn.Module.get_parameter" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed explanation of this methodâ€™s functionality as well as how to correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span></dt> <dd class="field-odd"><p><strong>target</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.htmlstr" title="(in Python v3.11)"><em>str</em></a>) â€“ The fully-qualified string name of the Parameter to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a fully-qualified string.)</p> </dd> <dt class="field-even">Returns<span class="colon">:</span></dt> <dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p> </dd> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p>torch.nn.Parameter</p> </dd> <dt class="field-even">Raises<span class="colon">:</span></dt> <dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.htmlAttributeError" title="(in Python v3.11)"><strong>AttributeError</strong></a> â€“ If the target string references an invalid     path or resolves to something that is not an     <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p> </dd> </dl><br></td></tr><tr><td class="indextd">1</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.register_backward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="î§‹" href="torch.nn.Module.register_backward_hook" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of <a class="reference internal has-code" href="torch.nn.Module.register_full_backward_hook" title="torch.nn.Module.register_full_backward_hook"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code></a> and the behavior of this function will change in future versions.</p> <dl class="field-list simple"> <dt class="field-odd">Returns<span class="colon">:</span></dt> <dd class="field-odd"><p>a handle that can be used to remove the added hook by calling <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p> </dd> <dt class="field-even">Return type<span class="colon">:</span></dt> <dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p> </dd> </dl><br></td></tr><tr><td class="indextd">2</td><td> torch<br><br> torch code opt: how to switch memory allocator?</td><td class="fixwtd">export LD_PRELOAD=<jemalloc.so/tcmalloc.so>:$LD_PRELOAD<br><br></td></tr><tr><td class="indextd">3</td><td>1 miles = ? Feet</td><td class="fixwtd">5280</td></tr><tr><td class="indextd">4</td><td><img src="/static/onepager/anki_images/paste-431bb3e432069b1467c2892ab418475dc8faa9eb.jpg"><br>è¿™æ˜¯ä»€ä¹ˆç½‘ç»œç»“æ„</td><td class="fixwtd">Alexnet<br></td></tr><tr><td class="indextd">5</td><td>Compare <u><b>squared L2 norm</b></u> and L2 norm,<br><br>Pro/con of its derivative?</td><td class="fixwtd"><div>Another advantage of the squared L2&nbsp;norm is that its partial derivative is easily computed:</div><img src="/static/onepager/anki_images/paste-a23d3e51be2a5dccead7c01065472f2c539ca91e.jpg"><br><br>L2 norm derivative:<br><img src="/static/onepager/anki_images/paste-d7ad236e1bec0795566a9966def34233421fe12e.jpg"><br><br><span style="color: rgba(0, 0, 0, 0.87); background-color: rgb(255, 255, 255);">One problem of the squared L</span>2<span style="color: rgba(0, 0, 0, 0.87); background-color: rgb(255, 255, 255);">&nbsp;norm is that it hardly discriminates between 0 and small values because the increase of the function is slow.</span><br></td></tr><tr><td class="indextd">6</td><td>Cron * meaning?</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-9e9bb4b605f4bdfb24655109d760ac48dfeb5be1.png"></td></tr><tr><td class="indextd">7</td><td>Is optimal control and reinforcement learning the same thing?<div>Cmu 16-745 2023</div></td><td class="fixwtd">Yes&nbsp;<div>Math is same&nbsp;</div></td></tr><tr><td class="indextd">8</td><td>Laplace distribution vs. normal distribution&nbsp;</td><td class="fixwtd">1. For long tail distribution&nbsp;</td></tr><tr><td class="indextd">9</td><td>Makefile ç›¸æ¯”äºshellè„šæœ¬çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ</td><td class="fixwtd">1. æ‹“æ‰‘ç»“æ„ä¾èµ–å›¾ï¼Œéƒ¨åˆ†ç¼–è¯‘<div>2. å…é™¤if else åˆ†æ”¯ æœ‰å®</div></td></tr><tr><td class="indextd">10</td><td>Please provide 5 types of matrix decomposition:<br><img src="/static/onepager/anki_images/paste-f761132ab026da31944274e1840947d212d3e985.jpg"></td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-757ee3c0848b7b160c8bd7a855562f165b0d3ef6.jpg"></td></tr><tr><td class="indextd">11</td><td>Please use trace to represent Frobenius norm<br><img src="/static/onepager/anki_images/paste-69ffe0bb169f3bbc5d5a5598e6ea7738abd48eff.jpg"></td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-8994f35804cdecf3d814c834c18934726001ae1b.jpg"></td></tr><tr><td class="indextd">12</td><td>Please write the formula of p-norms of a vector x</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-8f9c36ae5eb922b5f62c72f71121260fcde8cfcd.jpg"></td></tr><tr><td class="indextd">13</td><td>The triangle inequality of vector:</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-44a292a37039860198b0395caaaadff8c8fc96f0.jpg"></td></tr><tr><td class="indextd">14</td><td>how to create matrix A=CR decomposition? (row/col echelon form)</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-d900785b6ca3dc1f7654979173579afe9b5c4a6c.jpg"><br><img src="/static/onepager/anki_images/paste-5345fc8f750895100f4a33a6d20931279aba639b.jpg"></td></tr><tr><td class="indextd">15</td><td>how to do matrix A=LU decomposition?</td><td class="fixwtd">sum of rank1 matricies.<br><br><img src="/static/onepager/anki_images/paste-bba6fffa7704c042ad7446aa0849deaf3b5bb88e.jpg"></td></tr><tr><td class="indextd">16</td><td>how to do matrix A=QR decomposition?</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-8dd11fc770c2d00418fd4ca5ff062e7233d18dfa.jpg"></td></tr><tr><td class="indextd">17</td><td>how to do matrix singular value decomposition?<br></td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-2a34eab522009fa1c34618d26427cdc04ffd8515.jpg"><br>properties:<br><img src="/static/onepager/anki_images/paste-4399c2a365480de8adb57d94fdb7a5df411822b1.jpg"></td></tr><tr><td class="indextd">18</td><td>how to do matrix[$$] A=Q\varLambda Q^T [/$$] eigen value decomposition?</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-ac384c82e97f2b05134e4f3e3665e6818085aff6.jpg"><br>properties:<br><img src="/static/onepager/anki_images/paste-2d41276b9d8db1605bbbc37906e7c62558c787ae.jpg"></td></tr><tr><td class="indextd">19</td><td>ä¸€å›½ç”Ÿæ´»æ°´å¹³å–å†³äº</td><td class="fixwtd">ç”Ÿäº§ç‰©å“å’ŒåŠ³åŠ¡çš„èƒ½åŠ› ä»¥åŠç”Ÿäº§ç‡</td></tr><tr><td class="indextd">20</td><td>äºšå½“æ–¯å¯†è¯´å¦‚æœä¸€ä»¶ä¸œè¥¿æ€ä¹ˆæ ·ï¼Œå°±æ°¸è¿œä¸æƒ³åœ¨å®¶é‡Œç”Ÿäº§ï¼Ÿ</td><td class="fixwtd">è´­ä¹°ä»£ä»·æ¯”å®¶é‡Œç”Ÿäº§å°</td></tr><tr><td class="indextd">21</td><td>å‡¯æ©æ–¯æå‡ºä¸–ç•Œæ˜¯æœ‰å°‘æ•°äººç»Ÿæ²»çš„ï¼Œé‚£äº›ç›¸ä¿¡è‡ªå·±æ™ºåŠ›ä¸Šä¸å—å½±å“çš„å®å¹²å®¶å¾€å¾€æ˜¯</td><td class="fixwtd">
å·²ç»è¿‡ä¸–çš„ç»æµå­¦å®¶çš„å¥´éš¶</td></tr><tr><td class="indextd">22</td><td>å®Œå…¨ç«äº‰å¸‚åœºæŒ‡&nbsp;</td><td class="fixwtd">æ‰€æä¾›çš„ç‰©å“æ˜¯ç›¸åŒçš„ï¼Œè€Œä¸”å–å®¶ä¹°å®¶æ•°é‡å¤šï¼Œæ— æ³•å½±å“ä»·æ ¼</td></tr><tr><td class="indextd">23</td><td>å¹¿å‘Šç½‘ç»œä¸­ä¸¤ç§ç«ä»·æ ‡çš„ç‰©<br><ol><li>{{c1::ä¸Šä¸‹æ–‡keyword}}</li><li>{{c2::ç”¨æˆ·å…´è¶£æ ‡ç­¾ ï¼ˆæ¥è‡ªäºå†å²è¡Œä¸ºï¼‰}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">24</td><td>å¹¿å‘Šç½‘ç»œä¸­ä¸¤ç§ç«ä»·æ ‡çš„ç‰©<br><ol><li>{{c1::ä¸Šä¸‹æ–‡keyword}}</li><li>{{c2::ç”¨æˆ·å…´è¶£æ ‡ç­¾ ï¼ˆæ¥è‡ªäºå†å²è¡Œä¸ºï¼‰}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">25</td><td>å¹¿å‘Šç½‘ç»œä½¿ç”¨eCPMè®¡è´¹çš„ä¸¤ä¸ªéš¾ç‚¹ï¼š<br><ol><li>{{c1::å†·å¯åŠ¨é—®é¢˜}}</li><li>{{c2::å¦‚ä½•å½’ä¸€åŒ–å¹¿å‘Šä½çš„å½±å“}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">26</td><td>å¹¿å‘Šç½‘ç»œä½¿ç”¨eCPMè®¡è´¹çš„ä¸¤ä¸ªéš¾ç‚¹ï¼š<br><ol><li>{{c1::å†·å¯åŠ¨é—®é¢˜}}</li><li>{{c2::å¦‚ä½•å½’ä¸€åŒ–å¹¿å‘Šä½çš„å½±å“}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">27</td><td>å¼¹æ€§æ˜¯æŒ‡ä¹°å®¶å¯¹ä»·æ ¼çš„æ•æ„Ÿç¨‹åº¦ ä»¥åŠ</td><td class="fixwtd">éœ€æ±‚é‡å¯¹ä»·æ ¼å˜åŠ¨çš„ååº”</td></tr><tr><td class="indextd">28</td><td>å¿…éœ€å“ç›¸æ¯”äºå¥¢ä¾ˆå“çš„å¼¹æ€§ï¼Ÿ</td><td class="fixwtd">å¥¢ä¾ˆå“å¼¹æ€§å¤§</td></tr><tr><td class="indextd">29</td><td>æ’é™¤å‹ç¬”è®°çš„ä¸»è¦ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ</td><td class="fixwtd">è®°å½•æ›¾ç»åšè¿‡çš„å®éªŒ/çœ‹è¿‡çš„æ–‡ç« /å¬è¿‡çš„è®²åº§, é¿å…é‡å¤å·¥ä½œ<div><br></div></td></tr><tr><td class="indextd">30</td><td>æŸç§ä¸œè¥¿çš„æœºä¼šæˆæœ¬æ˜¯</td><td class="fixwtd">ä¸ºäº†å¾—åˆ°è¿™ç§ä¸œè¥¿æ”¾å¼ƒçš„ä¸œè¥¿</td></tr><tr><td class="indextd">31</td><td>æ ¹æ®å¼¹æ€§ç†è®ºè§£é‡Šæ„¿æ„æ”¯æŒæ”¯ä»˜é«˜ä»·æ¢é€šå‹¤æ—¶é—´çš„äººï¼Ÿ</td><td class="fixwtd">é‚£äº›æ„¿æ„æ”¯ä»˜æœ€é«˜ä»·æ ¼ä»¥èŠ‚çœé€šå‹¤æ—¶é—´çš„äººå·²ç»é€šè¿‡æ­<div>ä¹˜å…¬å…±äº¤é€šè½¦ï¼Œæ¬åˆ°ç¦»å·¥ä½œå•ä½è¿‘çš„åœ°æ–¹ï¼Œæˆ–é€‰æ‹©å¯ä»¥</div><div>ä½¿ä»–ä»¬é¿å¼€äº¤é€šé«˜å³°çš„å·¥ä½œæ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚</div><div>ç›¸åçš„äººå·²ç»æœ‰å¾ˆé•¿çš„é€šå‹¤æ—¶é—´äº†</div></td></tr><tr><td class="indextd">32</td><td>æ¯”è¾ƒä¼˜åŠ¿æ˜¯æŒ‡ç”Ÿäº§ä¸€ç§ç‰©å“çš„ä»€ä¹ˆè¾ƒå°‘</td><td class="fixwtd">æœºä¼šæˆæœ¬</td></tr><tr><td class="indextd">33</td><td>ç”Ÿäº§ä¸€ç§ç‰©å“æ‰€éœ€æŠ•å…¥é‡è¾ƒå°‘ï¼Œå°±å¯ä»¥è¯´å…·æœ‰</td><td class="fixwtd">ç»å¯¹ä¼˜åŠ¿</td></tr><tr><td class="indextd">34</td><td>ç›¸è¿‘æ›¿ä»£å“å¤šçš„ç‰©å“å¼¹æ€§ï¼Ÿ</td><td class="fixwtd">ç›¸è¿‘æ›¿ä»£å“å¤šçš„ç‰©å“å¼¹æ€§å¤§</td></tr><tr><td class="indextd">35</td><td>èŒƒå›´å°çš„å¸‚åœºå¼¹æ€§ï¼Ÿä¾‹å¦‚é¦™è‰ğŸŒ¿å†°æ·‡æ·‹ğŸ¦</td><td class="fixwtd">èŒƒå›´å°å¸‚åœºå¼¹æ€§å¤§</td></tr><tr><td class="indextd">36</td><td>é‡ç‚¹å‹ç¬”è®°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ</td><td class="fixwtd">ä½¿ç”¨ankiå¤ä¹ è®°å¿†</td></tr><tr><td class="indextd">37</td><td> 2019, STD<br><br>Iverson bracket indicator å…¬å¼<br></td><td class="fixwtd">[p]=1 if p else 0<div>åˆ¤æ–­å‘½é¢˜pæ˜¯å¦ä¸ºçœŸ</div></td></tr><tr><td class="indextd">38</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">ipu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.ipu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="î§‹" href="torch.nn.Module.ipu" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p><br></p><p>IPUï¼š<a href="https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html"><i><i>Infrastructure</i>&nbsp;<i>Processing</i>&nbsp;<i>Unit</i>&nbsp;</i></a>&nbsp;for cloud computing.</p><p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This method modifies the module in-place.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span></dt> <dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.htmlint" title="(in Python v3.11)"><em>int</em></a><em>, </em><em>optional</em>) â€“ if specified, all parameters will be copied to that device</p> </dd> <dt class="field-even">Returns<span class="colon">:</span></dt> <dd class="field-even"><p>self</p> </dd> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p><a class="reference internal" href="torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl><br></td></tr><tr><td class="indextd">39</td><td> torch/tikz/python<br><br> how to use torch util benchmark?</td><td class="fixwtd"><br>num_threads = torch.get_num_threads()<br>print(f'Benchmarking on {num_threads} threads')<br><br>t0 = benchmark.Timer(<br>stmt='batched_dot_mul_sum(x, x)',<br>setup='from __main__ import batched_dot_mul_sum',<br>globals={'x': x},<br>num_threads=num_threads,<br>label='Multithreaded batch dot',<br>sub_label='Implemented using mul and sum')<br><br>t1 = benchmark.Timer(<br>stmt='batched_dot_bmm(x, x)',<br>setup='from __main__ import batched_dot_bmm',<br>globals={'x': x},<br>num_threads=num_threads,<br>label='Multithreaded batch dot',<br>sub_label='Implemented using bmm')<br><br>print(t0.timeit(100))<br>print(t1.timeit(100))<br><br><br></td></tr><tr><td class="indextd">40</td><td> torch/tikz/python<br><br> what is quantized into int8 when using QAT and post static qunatization?</td><td class="fixwtd"><br>%original model<br>%all tensors and computations are in floating point<br>previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32<br>/<br>linear_weight_fp32<br><br>%model with fake_quants for modeling quantization numerics during training<br>previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32<br>/<br>linear_weight_fp32 -- fq<br><br>%quantized model<br>%weights and activations are in int8<br>previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8<br>/<br>linear_weight_int8<br><br><br></td></tr><tr><td class="indextd">41</td><td> torch<br><br> After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. What is the solution?</td><td class="fixwtd"><br>replace your lists / dicts in Dataloader __getitem__ with numpy arrays.<br><br><br><br><br><br></td></tr><tr><td class="indextd">42</td><td>è®¡ç®—å¹¿å‘Š<div>åœ¨çº¿å¹¿å‘Šçš„è¡¨ç°/å®ç°å½¢å¼æœ‰å‡ ç§</div><div><br></div><div>åˆ†ä¸ºä¸‰ç»„ï¼Œåˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ</div><div>è¿›åŒ–ç»„</div><div>è®¾å¤‡ç»„</div><div>æ¿€åŠ±ç»„</div><div>è”è¿ç»„</div></td><td class="fixwtd">12ç§<div><font color="#ffd60a"><u>è¿›åŒ–ç»„</u></font><br><div>1. æ¨ªå¹…å¹¿å‘Š banner ad</div><div>2. æ–‡å­—é“¾å¹¿å‘Šï¼Œ<font color="#ff9f0a">egæœç´¢å¹¿å‘Šsponsored search,</font> <font color="#0a84ff">contextual ad&nbsp;</font></div><div>3. å¯Œåª’ä½“å¹¿å‘Š ï¼Œegå¼¹çª—ï¼Œå¯¹è”ï¼Œå…¨å±</div><div>4. è§†é¢‘å¹¿å‘Š</div><div>____________________</div><div>å›¾1-4 å¯Œåª’ä½“å¹¿å‘Šç¤ºä¾‹</div><div>è§†é¢‘å¹¿å‘Šæœ‰å‡ ç§ä¸»è¦çš„å½¢å¼ã€‚</div><div>. åœ¨è§†é¢‘å†…å®¹æ’­æ”¾ä¹‹å‰çš„å‰æ’ç‰‡å¹¿å‘Šã€‚æ ¹æ®æ’å…¥ä½ç½®çš„ä¸åŒï¼Œè§†é¢‘å¹¿å‘Šåˆå¯ä»¥åˆ†ä¸º<font color="#ffd60a">å‰æ’ç‰‡ã€åæ’ç‰‡ã€æš‚åœ</font>ç­‰ç±»å‹ã€‚å›¾1-5ç»™å‡ºäº†è§†é¢‘å¹¿å‘Šçš„ç¤ºä¾‹ã€‚è§†é¢‘å¹¿å‘Šç”±äºè½½ä½“çš„ç‹¬ç‰¹æ€§è´¨ï¼Œå…¶æ•ˆæœå’Œå¹¿å‘Šåˆ›æ„ä¼šæ¯”è¾ƒç±»ä¼¼äºçº¿ä¸‹çš„ç”µè§†å¹¿å‘Šã€‚è¿™ç§å¹¿å‘Šä¸€èˆ¬é‡‡ç”¨çŸ­è§†é¢‘çš„å½¢å¼ï¼Œåˆ›æ„çš„è¡¨ç°åŠ›è¦è¿œè¿œå¼ºäºæ™®é€šçš„å±•ç¤ºå¹¿å‘Šï¼Œå› æ­¤ä»·æ ¼å¾€å¾€ä¹Ÿæ¯”è¾ƒé«˜ã€‚</div><div>. åœ¨ä¿¡æ¯æµä¸­æ’å…¥çš„è§†é¢‘å¹¿å‘Šã€‚åœ¨Wi-Fiåœºæ™¯ä¸‹å¾€å¾€<font color="#ffd60a">è‡ªåŠ¨æ’­æ”¾</font>ï¼Œå…¶æ•ˆæœè¿œä¼˜äºæ™®é€šçš„ä¿¡æ¯æµå±•ç¤ºå¹¿å‘Šã€‚</div><div>. æ‰‹æœºæ¸¸æˆä¸­çš„æ¿€åŠ±è§†é¢‘å¹¿å‘Šã€‚å®ƒä¸»è¦æ˜¯åˆ©ç”¨æ¸¸æˆä¸­<font color="#ffd60a">çš„ç§¯åˆ†å¥–åŠ±</font>ï¼Œåˆºæ¿€ç”¨æˆ·ä¸»åŠ¨è§‚çœ‹è§†é¢‘å¹¿å‘Šï¼Œè¿™ç§å¹¿å‘Šå¾€å¾€è§‚çœ‹ç‡è¾ƒé«˜ï¼Œå¹¿å‘Šæ•ˆæœä¹Ÿè¾ƒå¥½ã€‚</div><div>______________</div><div><font color="#ffd60a"><u>è®¾å¤‡ç»„</u></font></div><div>5. äº¤äº’å¼å¹¿å‘Šï¼Œä¸ä¸‹è½½APPä½“éªŒ</div><div>6. ç¤¾äº¤å¹¿å‘Šï¼ŒTwitterä¿¡æ¯æµå¹¿å‘Š</div><div>7. ç§»åŠ¨ç«¯å¹¿å‘Šï¼Œæ¨ªå¹…ï¼Œå¼€å±ï¼Œæ’å±ï¼Œç§¯åˆ†å¢™ï¼Œæ¨èå¢™</div><div>8. é‚®ä»¶è¥é”€å¹¿å‘Šï¼Œä¸»åŠ¨å¹¿å‘Šï¼ŒèŠ‚åˆ¶é˜²åƒåœ¾</div><div><u><font color="#ffd60a">æ¿€åŠ±ç»„</font></u></div><div>9. æ¿€åŠ±å¹¿å‘Š incentive ADï¼Œç§¯åˆ†å¢™ï¼Œè¿”åˆ©å¹¿å‘Š</div><div>10. å›¢è´­</div><div><br></div><div><font color="#ffd60a"><u>è”è¿ç»„</u></font></div><div>11. æ¸¸æˆè”è¿ï¼Œä¸‹è½½ä¸“åŒºï¼ŒAPP store</div><div>12. å›ºå®šä½å¯¼èˆªï¼Œæ©±çª—æ•ˆåº”ï¼Œåˆçº¦å¹¿å‘Š</div><div><br></div></div></td></tr><tr><td class="indextd">43</td><td> torch<br><br> pytorch distributed: main source of training latency?</td><td class="fixwtd">1. communication delay<br>2. bucket size<br>3. skip sync<br><br><br></td></tr><tr><td class="indextd">44</td><td>LSTM structure&nbsp;</td><td class="fixwtd"><img src="/static/onepager/anki_images/image-556fb2ece0c8d76fa4da57a8b7519a355e90f9c2.png"><div>C = tanh (input, h)</div><div>Output = sigma * tanh (input , h)</div><div>H=output * tanh C</div></td></tr><tr><td class="indextd">45</td><td>torch.broadcast_to&nbsp;&nbsp;</td><td class="fixwtd"><section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap"><div class="pytorch-content-left"><div class="rst-content"><div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main"><article class="pytorch-article" id="pytorch-article" itemprop="articleBody"><section id="torch-broadcast-to"><h1>torch.broadcast_to<a class="headerlink" href="#torch-broadcast-to" title="Permalink to this heading">Â¶</a></h1><dl class="py function"><dt class="sig sig-object py" id="torch.broadcast_to"><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">broadcast_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">â†’</span> <span class="sig-return-typehint"><a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#torch.broadcast_to" title="Permalink to this definition">Â¶</a></dt><dd><p>Broadcasts <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the shape <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.Equivalent to calling <code class="docutils literal notranslate"><span class="pre">input.expand(shape)</span></code>. See <a class="reference internal" href="torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a> for details.</p><dl class="field-list simple"><dt class="field-odd">Parameters<span class="colon">:</span></dt><dd class="field-odd"><ul class="simple"><li><p><strong>input</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) â€“ the input tensor.</p></li><li><p><strong>shape</strong> (list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>) â€“ the new shape.</p></li></ul></dd></dl><p>Example:</p><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="go">tensor([[1, 2, 3],</span><span class="go">        [1, 2, 3],</span><span class="go">        [1, 2, 3]])</span></pre></div></div></dd></dl></section></article></div><footer><div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">&nbsp;</div></footer></div></div></section></td></tr><tr><td class="indextd">46</td><td>
<div>
<div>
<div>&nbsp;Eigenvalues and Eigenvectors<br>Systems of Differential Equations&nbsp;</div><div>ä½¿ç”¨ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡è¡¨ç¤ºå·®åˆ†æ–¹ç¨‹è§£çš„å½¢å¼ï¼Ÿ</div><div><img src="/static/onepager/anki_images/paste-7e9d70ecb7e01826d554837c6c1b3556eb33b469.jpg"><br></div><div><br></div><div><img src="/static/onepager/anki_images/paste-85034bda47d725cdb648a4340e6bf3555ad8a68c.jpg"><br></div><div><br></div>
</div>
</div></td><td class="fixwtd">
<div>
<div>
<div><img src="/static/onepager/anki_images/paste-9fcca00f5d4726a0bb3210436a9b28700c06d737.jpg"><br></div><div><img src="/static/onepager/anki_images/paste-360c2124d0b937e6c43c89dd187912259340fa68.jpg"><br></div><div><br></div><div><br></div><div><img src="/static/onepager/anki_images/paste-ee67dad61addc1a1176330a2650c969a373b079a.jpg"><br></div>
</div>
</div></td></tr><tr><td class="indextd">47</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.extra_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="î§‹" href="torch.nn.Module.extra_repr" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> <dl class="field-list simple"> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.htmlstr" title="(in Python v3.11)">str</a></p> </dd> </dl><br></td></tr><tr><td class="indextd">48</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="î§‹" href="torch.nn.Module.parameters" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span></dt> <dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.htmlbool" title="(in Python v3.11)"><em>bool</em></a>) â€“ if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p> </dd> <dt class="field-even">Yields<span class="colon">:</span></dt> <dd class="field-even"><p><em>Parameter</em> â€“ module parameter</p> </dd> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.htmltyping.Iterator" title="(in Python v3.11)"><em>Iterator</em></a>[<a class="reference internal" href="torch.nn.parameter.Parameter.htmltorch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><em>Parameter</em></a>]</p> </dd> </dl> <p>Example:</p> <div class="highlight-default notranslate"><div class="highlight"><pre id="codecell9"><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="go">&lt;class 'torch.Tensor'&gt; (20L,)</span> <span class="go">&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</span> </pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="codecell9">       <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="000000" fill="none" stroke-linecap="round" stroke-linejoin="round">   <title>Copy to clipboard</title>   <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>   <rect x="8" y="8" width="12" height="12" rx="2"></rect>   <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path> </svg>     </button></div> </div><br></td></tr><tr><td class="indextd">49</td><td>CPTå¸¸ç”¨ä»€ä¹ˆç³»ç»Ÿæ’æœŸï¼Ÿ</td><td class="fixwtd">1. double clickçš„DFP<div>2. ä¸­å›½å¥½è€¶allyes</div><div>3. ç™¾åº¦å¹¿å‘Šç®¡å®¶</div><div><br></div></td></tr>
        </table>
    </body>
    