
    <head>
        <style>
            table {
                --color: #d0d0f5;
            }

            thead,
            tfoot {
                background: var(--color);
            }

            tbody tr:nth-child(even) {
                background: color-mix(in srgb, var(--color), transparent 60%);
            }
            div, p, td {
                width: 200px;
                word-wrap: break-word;
                white-space: normal;
            }
            .fixwtd {
                width: 200px;
                word-wrap: break-word;
                white-space: normal;
            }
            .indextd {
                width: 10px;
                word-wrap: break-word;
                white-space: normal;
            }
        </style>
    </head>
    <body>
        <table>
          <tr><td class="indextd">0</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.get_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="" href="torch.nn.Module.get_parameter" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed explanation of this method’s functionality as well as how to correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span></dt> <dd class="field-odd"><p><strong>target</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.htmlstr" title="(in Python v3.11)"><em>str</em></a>) – The fully-qualified string name of the Parameter to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a fully-qualified string.)</p> </dd> <dt class="field-even">Returns<span class="colon">:</span></dt> <dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p> </dd> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p>torch.nn.Parameter</p> </dd> <dt class="field-even">Raises<span class="colon">:</span></dt> <dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.htmlAttributeError" title="(in Python v3.11)"><strong>AttributeError</strong></a> – If the target string references an invalid     path or resolves to something that is not an     <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p> </dd> </dl><br></td></tr><tr><td class="indextd">1</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.register_backward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="" href="torch.nn.Module.register_backward_hook" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of <a class="reference internal has-code" href="torch.nn.Module.register_full_backward_hook" title="torch.nn.Module.register_full_backward_hook"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code></a> and the behavior of this function will change in future versions.</p> <dl class="field-list simple"> <dt class="field-odd">Returns<span class="colon">:</span></dt> <dd class="field-odd"><p>a handle that can be used to remove the added hook by calling <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p> </dd> <dt class="field-even">Return type<span class="colon">:</span></dt> <dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p> </dd> </dl><br></td></tr><tr><td class="indextd">2</td><td> torch<br><br> torch code opt: how to switch memory allocator?</td><td class="fixwtd">export LD_PRELOAD=<jemalloc.so/tcmalloc.so>:$LD_PRELOAD<br><br></td></tr><tr><td class="indextd">3</td><td>1 miles = ? Feet</td><td class="fixwtd">5280</td></tr><tr><td class="indextd">4</td><td><img src="/static/onepager/anki_images/paste-431bb3e432069b1467c2892ab418475dc8faa9eb.jpg"><br>这是什么网络结构</td><td class="fixwtd">Alexnet<br></td></tr><tr><td class="indextd">5</td><td>Compare <u><b>squared L2 norm</b></u> and L2 norm,<br><br>Pro/con of its derivative?</td><td class="fixwtd"><div>Another advantage of the squared L2&nbsp;norm is that its partial derivative is easily computed:</div><img src="/static/onepager/anki_images/paste-a23d3e51be2a5dccead7c01065472f2c539ca91e.jpg"><br><br>L2 norm derivative:<br><img src="/static/onepager/anki_images/paste-d7ad236e1bec0795566a9966def34233421fe12e.jpg"><br><br><span style="color: rgba(0, 0, 0, 0.87); background-color: rgb(255, 255, 255);">One problem of the squared L</span>2<span style="color: rgba(0, 0, 0, 0.87); background-color: rgb(255, 255, 255);">&nbsp;norm is that it hardly discriminates between 0 and small values because the increase of the function is slow.</span><br></td></tr><tr><td class="indextd">6</td><td>Cron * meaning?</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-9e9bb4b605f4bdfb24655109d760ac48dfeb5be1.png"></td></tr><tr><td class="indextd">7</td><td>Is optimal control and reinforcement learning the same thing?<div>Cmu 16-745 2023</div></td><td class="fixwtd">Yes&nbsp;<div>Math is same&nbsp;</div></td></tr><tr><td class="indextd">8</td><td>Laplace distribution vs. normal distribution&nbsp;</td><td class="fixwtd">1. For long tail distribution&nbsp;</td></tr><tr><td class="indextd">9</td><td>Makefile 相比于shell脚本的优势是什么</td><td class="fixwtd">1. 拓扑结构依赖图，部分编译<div>2. 免除if else 分支 有宏</div></td></tr><tr><td class="indextd">10</td><td>Please provide 5 types of matrix decomposition:<br><img src="/static/onepager/anki_images/paste-f761132ab026da31944274e1840947d212d3e985.jpg"></td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-757ee3c0848b7b160c8bd7a855562f165b0d3ef6.jpg"></td></tr><tr><td class="indextd">11</td><td>Please use trace to represent Frobenius norm<br><img src="/static/onepager/anki_images/paste-69ffe0bb169f3bbc5d5a5598e6ea7738abd48eff.jpg"></td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-8994f35804cdecf3d814c834c18934726001ae1b.jpg"></td></tr><tr><td class="indextd">12</td><td>Please write the formula of p-norms of a vector x</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-8f9c36ae5eb922b5f62c72f71121260fcde8cfcd.jpg"></td></tr><tr><td class="indextd">13</td><td>The triangle inequality of vector:</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-44a292a37039860198b0395caaaadff8c8fc96f0.jpg"></td></tr><tr><td class="indextd">14</td><td>how to create matrix A=CR decomposition? (row/col echelon form)</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-d900785b6ca3dc1f7654979173579afe9b5c4a6c.jpg"><br><img src="/static/onepager/anki_images/paste-5345fc8f750895100f4a33a6d20931279aba639b.jpg"></td></tr><tr><td class="indextd">15</td><td>how to do matrix A=LU decomposition?</td><td class="fixwtd">sum of rank1 matricies.<br><br><img src="/static/onepager/anki_images/paste-bba6fffa7704c042ad7446aa0849deaf3b5bb88e.jpg"></td></tr><tr><td class="indextd">16</td><td>how to do matrix A=QR decomposition?</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-8dd11fc770c2d00418fd4ca5ff062e7233d18dfa.jpg"></td></tr><tr><td class="indextd">17</td><td>how to do matrix singular value decomposition?<br></td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-2a34eab522009fa1c34618d26427cdc04ffd8515.jpg"><br>properties:<br><img src="/static/onepager/anki_images/paste-4399c2a365480de8adb57d94fdb7a5df411822b1.jpg"></td></tr><tr><td class="indextd">18</td><td>how to do matrix[$$] A=Q\varLambda Q^T [/$$] eigen value decomposition?</td><td class="fixwtd"><img src="/static/onepager/anki_images/paste-ac384c82e97f2b05134e4f3e3665e6818085aff6.jpg"><br>properties:<br><img src="/static/onepager/anki_images/paste-2d41276b9d8db1605bbbc37906e7c62558c787ae.jpg"></td></tr><tr><td class="indextd">19</td><td>一国生活水平取决于</td><td class="fixwtd">生产物品和劳务的能力 以及生产率</td></tr><tr><td class="indextd">20</td><td>亚当斯密说如果一件东西怎么样，就永远不想在家里生产？</td><td class="fixwtd">购买代价比家里生产小</td></tr><tr><td class="indextd">21</td><td>凯恩斯提出世界是有少数人统治的，那些相信自己智力上不受影响的实干家往往是</td><td class="fixwtd">
已经过世的经济学家的奴隶</td></tr><tr><td class="indextd">22</td><td>完全竞争市场指&nbsp;</td><td class="fixwtd">所提供的物品是相同的，而且卖家买家数量多，无法影响价格</td></tr><tr><td class="indextd">23</td><td>广告网络中两种竞价标的物<br><ol><li>{{c1::上下文keyword}}</li><li>{{c2::用户兴趣标签 （来自于历史行为）}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">24</td><td>广告网络中两种竞价标的物<br><ol><li>{{c1::上下文keyword}}</li><li>{{c2::用户兴趣标签 （来自于历史行为）}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">25</td><td>广告网络使用eCPM计费的两个难点：<br><ol><li>{{c1::冷启动问题}}</li><li>{{c2::如何归一化广告位的影响}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">26</td><td>广告网络使用eCPM计费的两个难点：<br><ol><li>{{c1::冷启动问题}}</li><li>{{c2::如何归一化广告位的影响}}</li></ol></td><td class="fixwtd"></td></tr><tr><td class="indextd">27</td><td>弹性是指买家对价格的敏感程度 以及</td><td class="fixwtd">需求量对价格变动的反应</td></tr><tr><td class="indextd">28</td><td>必需品相比于奢侈品的弹性？</td><td class="fixwtd">奢侈品弹性大</td></tr><tr><td class="indextd">29</td><td>排除型笔记的主要作用是什么？</td><td class="fixwtd">记录曾经做过的实验/看过的文章/听过的讲座, 避免重复工作<div><br></div></td></tr><tr><td class="indextd">30</td><td>某种东西的机会成本是</td><td class="fixwtd">为了得到这种东西放弃的东西</td></tr><tr><td class="indextd">31</td><td>根据弹性理论解释愿意支持支付高价换通勤时间的人？</td><td class="fixwtd">那些愿意支付最高价格以节省通勤时间的人已经通过搭<div>乘公共交通车，搬到离工作单位近的地方，或选择可以</div><div>使他们避开交通高峰的工作来做到这一点。</div><div>相反的人已经有很长的通勤时间了</div></td></tr><tr><td class="indextd">32</td><td>比较优势是指生产一种物品的什么较少</td><td class="fixwtd">机会成本</td></tr><tr><td class="indextd">33</td><td>生产一种物品所需投入量较少，就可以说具有</td><td class="fixwtd">绝对优势</td></tr><tr><td class="indextd">34</td><td>相近替代品多的物品弹性？</td><td class="fixwtd">相近替代品多的物品弹性大</td></tr><tr><td class="indextd">35</td><td>范围小的市场弹性？例如香草🌿冰淇淋🍦</td><td class="fixwtd">范围小市场弹性大</td></tr><tr><td class="indextd">36</td><td>重点型笔记的作用是什么？</td><td class="fixwtd">使用anki复习记忆</td></tr><tr><td class="indextd">37</td><td> 2019, STD<br><br>Iverson bracket indicator 公式<br></td><td class="fixwtd">[p]=1 if p else 0<div>判断命题p是否为真</div></td></tr><tr><td class="indextd">38</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">ipu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.ipu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="" href="torch.nn.Module.ipu" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p><br></p><p>IPU：<a href="https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html"><i><i>Infrastructure</i>&nbsp;<i>Processing</i>&nbsp;<i>Unit</i>&nbsp;</i></a>&nbsp;for cloud computing.</p><p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This method modifies the module in-place.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span></dt> <dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.htmlint" title="(in Python v3.11)"><em>int</em></a><em>, </em><em>optional</em>) – if specified, all parameters will be copied to that device</p> </dd> <dt class="field-even">Returns<span class="colon">:</span></dt> <dd class="field-even"><p>self</p> </dd> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p><a class="reference internal" href="torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl><br></td></tr><tr><td class="indextd">39</td><td> torch/tikz/python<br><br> how to use torch util benchmark?</td><td class="fixwtd"><br>num_threads = torch.get_num_threads()<br>print(f'Benchmarking on {num_threads} threads')<br><br>t0 = benchmark.Timer(<br>stmt='batched_dot_mul_sum(x, x)',<br>setup='from __main__ import batched_dot_mul_sum',<br>globals={'x': x},<br>num_threads=num_threads,<br>label='Multithreaded batch dot',<br>sub_label='Implemented using mul and sum')<br><br>t1 = benchmark.Timer(<br>stmt='batched_dot_bmm(x, x)',<br>setup='from __main__ import batched_dot_bmm',<br>globals={'x': x},<br>num_threads=num_threads,<br>label='Multithreaded batch dot',<br>sub_label='Implemented using bmm')<br><br>print(t0.timeit(100))<br>print(t1.timeit(100))<br><br><br></td></tr><tr><td class="indextd">40</td><td> torch/tikz/python<br><br> what is quantized into int8 when using QAT and post static qunatization?</td><td class="fixwtd"><br>%original model<br>%all tensors and computations are in floating point<br>previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32<br>/<br>linear_weight_fp32<br><br>%model with fake_quants for modeling quantization numerics during training<br>previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32<br>/<br>linear_weight_fp32 -- fq<br><br>%quantized model<br>%weights and activations are in int8<br>previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8<br>/<br>linear_weight_int8<br><br><br></td></tr><tr><td class="indextd">41</td><td> torch<br><br> After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. What is the solution?</td><td class="fixwtd"><br>replace your lists / dicts in Dataloader __getitem__ with numpy arrays.<br><br><br><br><br><br></td></tr><tr><td class="indextd">42</td><td>计算广告<div>在线广告的表现/实现形式有几种</div><div><br></div><div>分为三组，分别是什么？</div><div>进化组</div><div>设备组</div><div>激励组</div><div>联运组</div></td><td class="fixwtd">12种<div><font color="#ffd60a"><u>进化组</u></font><br><div>1. 横幅广告 banner ad</div><div>2. 文字链广告，<font color="#ff9f0a">eg搜索广告sponsored search,</font> <font color="#0a84ff">contextual ad&nbsp;</font></div><div>3. 富媒体广告 ，eg弹窗，对联，全屏</div><div>4. 视频广告</div><div>____________________</div><div>图1-4 富媒体广告示例</div><div>视频广告有几种主要的形式。</div><div>. 在视频内容播放之前的前插片广告。根据插入位置的不同，视频广告又可以分为<font color="#ffd60a">前插片、后插片、暂停</font>等类型。图1-5给出了视频广告的示例。视频广告由于载体的独特性质，其效果和广告创意会比较类似于线下的电视广告。这种广告一般采用短视频的形式，创意的表现力要远远强于普通的展示广告，因此价格往往也比较高。</div><div>. 在信息流中插入的视频广告。在Wi-Fi场景下往往<font color="#ffd60a">自动播放</font>，其效果远优于普通的信息流展示广告。</div><div>. 手机游戏中的激励视频广告。它主要是利用游戏中<font color="#ffd60a">的积分奖励</font>，刺激用户主动观看视频广告，这种广告往往观看率较高，广告效果也较好。</div><div>______________</div><div><font color="#ffd60a"><u>设备组</u></font></div><div>5. 交互式广告，不下载APP体验</div><div>6. 社交广告，Twitter信息流广告</div><div>7. 移动端广告，横幅，开屏，插屏，积分墙，推荐墙</div><div>8. 邮件营销广告，主动广告，节制防垃圾</div><div><u><font color="#ffd60a">激励组</font></u></div><div>9. 激励广告 incentive AD，积分墙，返利广告</div><div>10. 团购</div><div><br></div><div><font color="#ffd60a"><u>联运组</u></font></div><div>11. 游戏联运，下载专区，APP store</div><div>12. 固定位导航，橱窗效应，合约广告</div><div><br></div></div></td></tr><tr><td class="indextd">43</td><td> torch<br><br> pytorch distributed: main source of training latency?</td><td class="fixwtd">1. communication delay<br>2. bucket size<br>3. skip sync<br><br><br></td></tr><tr><td class="indextd">44</td><td>LSTM structure&nbsp;</td><td class="fixwtd"><img src="/static/onepager/anki_images/image-556fb2ece0c8d76fa4da57a8b7519a355e90f9c2.png"><div>C = tanh (input, h)</div><div>Output = sigma * tanh (input , h)</div><div>H=output * tanh C</div></td></tr><tr><td class="indextd">45</td><td>torch.broadcast_to&nbsp;&nbsp;</td><td class="fixwtd"><section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap"><div class="pytorch-content-left"><div class="rst-content"><div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main"><article class="pytorch-article" id="pytorch-article" itemprop="articleBody"><section id="torch-broadcast-to"><h1>torch.broadcast_to<a class="headerlink" href="#torch-broadcast-to" title="Permalink to this heading">¶</a></h1><dl class="py function"><dt class="sig sig-object py" id="torch.broadcast_to"><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">broadcast_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#torch.broadcast_to" title="Permalink to this definition">¶</a></dt><dd><p>Broadcasts <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the shape <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.Equivalent to calling <code class="docutils literal notranslate"><span class="pre">input.expand(shape)</span></code>. See <a class="reference internal" href="torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a> for details.</p><dl class="field-list simple"><dt class="field-odd">Parameters<span class="colon">:</span></dt><dd class="field-odd"><ul class="simple"><li><p><strong>input</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p></li><li><p><strong>shape</strong> (list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>) – the new shape.</p></li></ul></dd></dl><p>Example:</p><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="go">tensor([[1, 2, 3],</span><span class="go">        [1, 2, 3],</span><span class="go">        [1, 2, 3]])</span></pre></div></div></dd></dl></section></article></div><footer><div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">&nbsp;</div></footer></div></div></section></td></tr><tr><td class="indextd">46</td><td>
<div>
<div>
<div>&nbsp;Eigenvalues and Eigenvectors<br>Systems of Differential Equations&nbsp;</div><div>使用特征值和特征向量表示差分方程解的形式？</div><div><img src="/static/onepager/anki_images/paste-7e9d70ecb7e01826d554837c6c1b3556eb33b469.jpg"><br></div><div><br></div><div><img src="/static/onepager/anki_images/paste-85034bda47d725cdb648a4340e6bf3555ad8a68c.jpg"><br></div><div><br></div>
</div>
</div></td><td class="fixwtd">
<div>
<div>
<div><img src="/static/onepager/anki_images/paste-9fcca00f5d4726a0bb3210436a9b28700c06d737.jpg"><br></div><div><img src="/static/onepager/anki_images/paste-360c2124d0b937e6c43c89dd187912259340fa68.jpg"><br></div><div><br></div><div><br></div><div><img src="/static/onepager/anki_images/paste-ee67dad61addc1a1176330a2650c969a373b079a.jpg"><br></div>
</div>
</div></td></tr><tr><td class="indextd">47</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.extra_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="" href="torch.nn.Module.extra_repr" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> <dl class="field-list simple"> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.htmlstr" title="(in Python v3.11)">str</a></p> </dd> </dl><br></td></tr><tr><td class="indextd">48</td><td> nn.Module<br><br>  <span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal has-code" href="../_modules/torch/nn/modules/module.htmlModule.parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="" href="torch.nn.Module.parameters" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></td><td class="fixwtd"><p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span></dt> <dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.htmlbool" title="(in Python v3.11)"><em>bool</em></a>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p> </dd> <dt class="field-even">Yields<span class="colon">:</span></dt> <dd class="field-even"><p><em>Parameter</em> – module parameter</p> </dd> <dt class="field-odd">Return type<span class="colon">:</span></dt> <dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.htmltyping.Iterator" title="(in Python v3.11)"><em>Iterator</em></a>[<a class="reference internal" href="torch.nn.parameter.Parameter.htmltorch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><em>Parameter</em></a>]</p> </dd> </dl> <p>Example:</p> <div class="highlight-default notranslate"><div class="highlight"><pre id="codecell9"><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="go">&lt;class 'torch.Tensor'&gt; (20L,)</span> <span class="go">&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</span> </pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="codecell9">       <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="000000" fill="none" stroke-linecap="round" stroke-linejoin="round">   <title>Copy to clipboard</title>   <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>   <rect x="8" y="8" width="12" height="12" rx="2"></rect>   <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path> </svg>     </button></div> </div><br></td></tr><tr><td class="indextd">49</td><td>CPT常用什么系统排期？</td><td class="fixwtd">1. double click的DFP<div>2. 中国好耶allyes</div><div>3. 百度广告管家</div><div><br></div></td></tr>
        </table>
    </body>
    